{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DOC\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import count\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import asc\n",
    "from pyspark.sql.functions import avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    '''Create a spark session'''\n",
    "    spark = SparkSession \\\n",
    "    .builder\\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"MODEC\") \\\n",
    "    .config(\"spark.executor.memory\", \"2gb\") \\\n",
    "    .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "\n",
    "\n",
    "def read_gold(spark, file_path):\n",
    "    '''\n",
    "    Read parquet files\n",
    "    on the silver path\n",
    "\n",
    "    Parameters:\n",
    "    spark : Spark Session\n",
    "    file_path (str): Path to input data\n",
    "    '''\n",
    "    try:\n",
    "        df_data = spark.read.parquet(file_path)\n",
    "        return df_data\n",
    "    except IOError:\n",
    "        print('read error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data\n",
    "fact =read_gold(spark, 'datalake/gold/fact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+--------+----------+-------------------+-----+-----------+---------+---+----+-----+----+-------+\n",
      "|sensor_id|equipment_id|    code|group_name|               date|error|temperature|vibration|day|week|month|year|weekday|\n",
      "+---------+------------+--------+----------+-------------------+-----+-----------+---------+---+----+-----+----+-------+\n",
      "|       67|           1|5310B9D7|  FGHQWR2Q|2020-02-25 08:38:54|    1|     167.59|  9643.11| 25|   9|    2|2020|      3|\n",
      "|       67|           1|5310B9D7|  FGHQWR2Q|2020-03-04 00:04:46|    1|     204.48|   2184.2|  4|  10|    3|2020|      4|\n",
      "|       67|           1|5310B9D7|  FGHQWR2Q|2020-01-24 23:54:11|    1|     446.09| -2260.74| 24|   4|    1|2020|      6|\n",
      "|       67|           1|5310B9D7|  FGHQWR2Q|2019-12-24 19:50:00|    1|      186.6|  5810.96| 24|  52|   12|2019|      3|\n",
      "|       67|           1|5310B9D7|  FGHQWR2Q|2019-12-21 02:28:31|    1|     487.57| -1597.33| 21|  51|   12|2019|      7|\n",
      "+---------+------------+--------+----------+-------------------+-----+-----------+---------+---+----+-----+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#display data\n",
    "fact.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a 11645 failures in Jan/2020\n"
     ]
    }
   ],
   "source": [
    "#Question 1 - Total equipment failures that happened in Jan/2020?\n",
    "q1 = fact.where( (col('year') == 2020) & (col('month') == 1) & (col('error') == 1))\n",
    "print('There are a {0} failures in Jan/2020'.format(q1.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|result|\n",
      "+------+\n",
      "| 11645|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#IF you prefer SQL\n",
    "fact.createOrReplaceTempView('fact')\n",
    "q1_sql = spark.sql('SELECT COUNT(*) as result FROM fact\\\n",
    "               WHERE year = 2020 AND month = 1 AND error = 1')\n",
    "q1_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|    code|count|\n",
      "+--------+-----+\n",
      "|E1AD07D4| 1377|\n",
      "+--------+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------+------+\n",
      "|    CODE|result|\n",
      "+--------+------+\n",
      "|E1AD07D4|  1377|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Question 2 - Which equipment code had most failures in Jan/2020?\n",
    "\n",
    "#q1 has only jan 2020 results\n",
    "q2 = q1.groupBy('code').agg(count('code').alias('count')).orderBy(desc('count'))\n",
    "q2.show(1)\n",
    "\n",
    "#IN SQL\n",
    "#fact 'temp_table' has all results\n",
    "\n",
    "q2_sql = spark.sql('SELECT CODE,COUNT(CODE) as result FROM fact \\\n",
    "                    WHERE year = 2020 AND month = 1 AND error = 1 \\\n",
    "                    GROUP BY (CODE) ORDER BY (result) DESC LIMIT 1')\n",
    "q2_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+\n",
      "|group_name| avg|\n",
      "+----------+----+\n",
      "|  Z9K1SAP4|1161|\n",
      "|  VAPQY59S|1666|\n",
      "|  PA92NCXZ|1715|\n",
      "|  9N127Z5P|1732|\n",
      "|  NQWPA8D3|1747|\n",
      "|  FGHQWR2Q|3624|\n",
      "+----------+----+\n",
      "\n",
      "+----------+----+\n",
      "|group_name| avg|\n",
      "+----------+----+\n",
      "|  Z9K1SAP4|1161|\n",
      "|  VAPQY59S|1666|\n",
      "|  PA92NCXZ|1715|\n",
      "|  9N127Z5P|1732|\n",
      "|  NQWPA8D3|1747|\n",
      "|  FGHQWR2Q|3624|\n",
      "+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Question 3 - \n",
    "#Average amount of failures across equipment group, ordering by the amount of failures in ascending order?\n",
    "\n",
    "\n",
    "#q1 has only jan 2020 results\n",
    "#each record is equal an one error, so we count count to get the avg\n",
    "q3 = q1.groupBy('group_name').agg(count('group_name').alias('avg')).orderBy(asc('avg'))\n",
    "q3.show(100)\n",
    "\n",
    "\n",
    "# IN SQL\n",
    "q3_sql = spark.sql('SELECT group_name,COUNT(group_name) as avg FROM fact \\\n",
    "                    WHERE year = 2020 AND month = 1 AND error = 1 \\\n",
    "                    GROUP BY (group_name) ORDER BY (avg) ASC')\n",
    "\n",
    "q3_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
